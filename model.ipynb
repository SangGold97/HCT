{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from lifelines.utils import concordance_index\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "X = data.drop(['efs_time', 'efs', 'ID'], axis=1) \n",
    "y_time = data['efs_time']\n",
    "y_event = data['efs']\n",
    "\n",
    "# test data\n",
    "data_test = pd.read_csv('test.csv')\n",
    "X_test = data_test.drop(columns=['ID'], axis=1)\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        value_counts = df[col].value_counts()\n",
    "        category_map = {cat: idx for idx, cat in enumerate(value_counts.index)}\n",
    "        df[col] = df[col].map(category_map)\n",
    "        df[col] = df[col].astype('category')\n",
    "    \n",
    "    df['year_hct'] = df['year_hct'] - 2000\n",
    "    return df\n",
    "\n",
    "X = preprocess_data(X)\n",
    "X_test = preprocess_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bisect\n",
    "\n",
    "class CindexObjectiveSurrogate:\n",
    "    def __init__(self, time, event):\n",
    "        \"\"\"\n",
    "        Build a list of valid pairs (i, j) such that sample i had an event (event==1)\n",
    "        and time[i] < time[j]. The surrogate loss is computed over these pairs.\n",
    "        \"\"\"\n",
    "        self.time = time\n",
    "        self.event = event\n",
    "        self.valid_pairs = []\n",
    "        n = len(time)\n",
    "        for i in range(n):\n",
    "            if event[i] == 1:  # i must have the event observed\n",
    "                for j in range(n):\n",
    "                    if time[i] < time[j]:\n",
    "                        self.valid_pairs.append((i, j))\n",
    "                        \n",
    "    def __call__(self, y_pred, dataset=None):\n",
    "        \"\"\"\n",
    "        For each valid pair (i, j), we use the surrogate loss:\n",
    "        \n",
    "            L_{ij} = log(1 + exp(-(y_pred[i] - y_pred[j])))\n",
    "        \n",
    "        The gradients and Hessians are computed from this loss.\n",
    "        \"\"\"\n",
    "        n = len(y_pred)\n",
    "        gradients = np.zeros(n)\n",
    "        hessians = np.zeros(n)\n",
    "        \n",
    "        # For numerical stability, we can later consider vectorized implementation\n",
    "        for (i, j) in self.valid_pairs:\n",
    "            delta = y_pred[i] - y_pred[j]\n",
    "            # Compute sigmoid over (r_j - r_i) using the logistic function.\n",
    "            # Note: sigmoid = 1/(1+exp(delta)) is equivalent to Ïƒ(r_j - r_i)\n",
    "            sigmoid_ij = 1.0 / (1.0 + np.exp(delta))\n",
    "            \n",
    "            # The derivative of L_ij with respect to y_pred[i] is -sigmoid_ij,\n",
    "            # and for y_pred[j] it is +sigmoid_ij.\n",
    "            gradients[i] -= sigmoid_ij\n",
    "            gradients[j] += sigmoid_ij\n",
    "            \n",
    "            # The second derivative for this pair is sigmoid*(1-sigmoid)\n",
    "            second_deriv = sigmoid_ij * (1 - sigmoid_ij)\n",
    "            hessians[i] += second_deriv\n",
    "            hessians[j] += second_deriv\n",
    "            \n",
    "        return gradients, hessians\n",
    "\n",
    "# Example usage in the objective function for LightGBM:\n",
    "def objective_surrogate(trial):\n",
    "    params = {\n",
    "        'metric': 'custom',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 25, 127),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-06, 10., log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-06, 10., log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': -1,\n",
    "        'num_threads': 4,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # ... Stratified K-fold and data splitting as in your current code ...\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    race_groups = X['race_group'].unique()\n",
    "    fold_scores = []\n",
    "    fold = -1\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, X['race_group']):\n",
    "        fold += 1\n",
    "        print(f'Running in fold {fold}...')\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_time_train, y_time_val = y_time.iloc[train_idx], y_time.iloc[val_idx]\n",
    "        y_event_train, y_event_val = y_event.iloc[train_idx], y_event.iloc[val_idx]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_time_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_time_val)\n",
    "\n",
    "        def cindex_eval(y_pred, data_val):\n",
    "            # Calculate stratified C-index for validation data\n",
    "            race_specific_scores = []\n",
    "            for race in race_groups:\n",
    "                race_mask = X_val['race_group'] == race\n",
    "                if sum(race_mask) > 1:  # Only calculate if we have at least 2 samples\n",
    "                    surv = np.array([(e, t) for e, t in zip(y_event_val[race_mask], y_time_val[race_mask])], \n",
    "                                    dtype=[('event', bool), ('time', float)])\n",
    "                    race_cindex = concordance_index_censored(surv['event'], surv['time'], y_pred[race_mask])[0]\n",
    "                    race_specific_scores.append(race_cindex)\n",
    "            stratified_cindex = np.mean(race_specific_scores) - np.std(race_specific_scores)\n",
    "            return 'stratified-c-index', stratified_cindex, True\n",
    "        \n",
    "        # Use our surrogate objective function here.\n",
    "        params['objective'] = CindexObjectiveSurrogate(y_time_train.values, y_event_train.values)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            num_boost_round=1000,\n",
    "            feval=cindex_eval,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        stratified_cindex = cindex_eval(y_pred, X_val)[1]\n",
    "        fold_scores.append(stratified_cindex)\n",
    "    \n",
    "    print('5 folds stratified C-index:', fold_scores)\n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_surrogate, n_trials=3, n_jobs=1)\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Objective Function with LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoxObjective:\n",
    "    def __init__(self, time, event):\n",
    "        self.time = time\n",
    "        self.event = event\n",
    "        self.sorted_indices = np.argsort(self.time)\n",
    "        self.sorted_time = self.time[self.sorted_indices]\n",
    "        self.sorted_event = self.event[self.sorted_indices]\n",
    "        self.event_positions_sorted = [i for i in range(len(self.sorted_event)) if self.sorted_event[i] == 1]\n",
    "\n",
    "    def __call__(self, y_pred, dataset=None):  # Added dataset parameter with default None\n",
    "        sorted_exp_f = np.exp(y_pred[self.sorted_indices])\n",
    "        sum_risk = np.cumsum(sorted_exp_f[::-1])[::-1]\n",
    "        \n",
    "        sum_1_over_sum_risk = []\n",
    "        sum_1_over_sum_risk_squared = []\n",
    "        for event_pos in self.event_positions_sorted:\n",
    "            sum_r = sum_risk[event_pos]\n",
    "            sum_1_over_sum_risk.append(1 / sum_r)\n",
    "            sum_1_over_sum_risk_squared.append(1 / sum_r**2)\n",
    "        \n",
    "        cum_sum_1_over_sum_risk = np.cumsum(sum_1_over_sum_risk)\n",
    "        cum_sum_1_over_sum_risk_squared = np.cumsum(sum_1_over_sum_risk_squared)\n",
    "        \n",
    "        gradients = np.zeros(len(y_pred))\n",
    "        hessians = np.zeros(len(y_pred))\n",
    "        sorted_position = {self.sorted_indices[k]: k for k in range(len(self.sorted_indices))}\n",
    "        \n",
    "        for k in range(len(y_pred)):\n",
    "            sorted_pos_k = sorted_position[k]\n",
    "            index = bisect.bisect_right(self.event_positions_sorted, sorted_pos_k)\n",
    "            sum_1_over_sum_risk_k = cum_sum_1_over_sum_risk[index-1] if index > 0 else 0\n",
    "            sum_1_over_sum_risk_squared_k = cum_sum_1_over_sum_risk_squared[index-1] if index > 0 else 0\n",
    "            \n",
    "            exp_f_k = np.exp(y_pred[k])\n",
    "            sum_term_grad = exp_f_k * sum_1_over_sum_risk_k\n",
    "            sum_term_hess = exp_f_k * sum_1_over_sum_risk_k - exp_f_k**2 * sum_1_over_sum_risk_squared_k\n",
    "            \n",
    "            gradient_k = - self.event[k] + sum_term_grad\n",
    "            hessian_k = sum_term_hess\n",
    "            \n",
    "            gradients[k] = gradient_k\n",
    "            hessians[k] = hessian_k\n",
    "            \n",
    "        return gradients, hessians\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "        'metric': 'custom',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 25, 127),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'max_bin': trial.suggest_int('max_bin', 3, 255),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-06, 10., log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-06, 10., log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'verbose': -1,\n",
    "        'num_threads': 4,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    race_groups = X['race_group'].unique()\n",
    "    fold_scores = []\n",
    "    fold = -1\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, X['race_group']):\n",
    "        fold += 1\n",
    "        print(f'Running in fold {fold}...')\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_time_train, y_time_val = y_time.iloc[train_idx], y_time.iloc[val_idx]\n",
    "        y_event_train, y_event_val = y_event.iloc[train_idx], y_event.iloc[val_idx]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label= y_time_train)\n",
    "        val_data = lgb.Dataset(X_val, label= y_time_val)\n",
    "\n",
    "        def cindex_eval(y_pred, data_val):\n",
    "            # Calculate stratified C-index for validation data\n",
    "            race_specific_scores = []\n",
    "            for race in race_groups:\n",
    "                race_mask = X_val['race_group'] == race\n",
    "                if sum(race_mask) > 1:  # Only calculate if we have at least 2 samples\n",
    "                    surv = np.array([(e, t) for e, t in zip(y_event_val[race_mask], y_time_val[race_mask])], \n",
    "                                dtype=[('event', bool), ('time', float)])\n",
    "                    race_cindex = concordance_index(surv['time'], -y_pred[race_mask], surv['event'])\n",
    "                    race_specific_scores.append(race_cindex)\n",
    "            \n",
    "            stratified_cindex = np.mean(race_specific_scores) - np.std(race_specific_scores)\n",
    "            return 'stratified-c-index', stratified_cindex, True\n",
    "        \n",
    "        params['objective'] = CoxObjective(y_time_train.values, y_event_train.values)\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            num_boost_round=1000,\n",
    "            feval=cindex_eval,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "        )\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate stratified C-index for fold evaluation\n",
    "        stratified_cindex = cindex_eval(y_pred, X_val)[1]\n",
    "        fold_scores.append(stratified_cindex)\n",
    "    \n",
    "    print('5 folds stratified C-index:', fold_scores)\n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-26 11:00:34,354] A new study created in memory with name: no-name-4c815de0-c9c8-4a37-bee7-060c9a9ddaa2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in fold 0...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's stratified-c-index: 0.671053\n",
      "Running in fold 1...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[776]\tvalid_0's stratified-c-index: 0.664565\n",
      "Running in fold 2...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[614]\tvalid_0's stratified-c-index: 0.672109\n",
      "Running in fold 3...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[984]\tvalid_0's stratified-c-index: 0.669044\n",
      "Running in fold 4...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-26 11:06:39,627] Trial 0 finished with value: 0.6704280886415624 and parameters: {'num_leaves': 26, 'max_depth': 7, 'max_bin': 178, 'learning_rate': 0.024701283706556516, 'reg_alpha': 2.2640045684783586e-05, 'reg_lambda': 2.866864083510606e-06, 'min_data_in_leaf': 55, 'feature_fraction': 0.9463253128280514, 'bagging_fraction': 0.8981480225202592, 'bagging_freq': 8}. Best is trial 0 with value: 0.6704280886415624.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[832]\tvalid_0's stratified-c-index: 0.675369\n",
      "5 folds stratified C-index: [np.float64(0.6710527002178232), np.float64(0.664565452217577), np.float64(0.6721085150747396), np.float64(0.6690443135295748), np.float64(0.675369462168098)]\n",
      "Running in fold 0...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[982]\tvalid_0's stratified-c-index: 0.665779\n",
      "Running in fold 1...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's stratified-c-index: 0.659575\n",
      "Running in fold 2...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[989]\tvalid_0's stratified-c-index: 0.673413\n",
      "Running in fold 3...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's stratified-c-index: 0.662404\n",
      "Running in fold 4...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-26 11:13:09,562] Trial 1 finished with value: 0.6669354039554529 and parameters: {'num_leaves': 32, 'max_depth': 9, 'max_bin': 245, 'learning_rate': 0.01105369021228455, 'reg_alpha': 0.011585450817545157, 'reg_lambda': 2.104775345738505, 'min_data_in_leaf': 23, 'feature_fraction': 0.8553374410563878, 'bagging_fraction': 0.9778146175573622, 'bagging_freq': 6}. Best is trial 0 with value: 0.6704280886415624.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[870]\tvalid_0's stratified-c-index: 0.673506\n",
      "5 folds stratified C-index: [np.float64(0.665779074388085), np.float64(0.6595745113166227), np.float64(0.6734133029584342), np.float64(0.6624043771488111), np.float64(0.6735057539653119)]\n",
      "Running in fold 0...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[306]\tvalid_0's stratified-c-index: 0.669684\n",
      "Running in fold 1...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[304]\tvalid_0's stratified-c-index: 0.66374\n",
      "Running in fold 2...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's stratified-c-index: 0.671538\n",
      "Running in fold 3...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's stratified-c-index: 0.661494\n",
      "Running in fold 4...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-26 11:15:17,929] Trial 2 finished with value: 0.6670483068478383 and parameters: {'num_leaves': 96, 'max_depth': 10, 'max_bin': 20, 'learning_rate': 0.04978326254933003, 'reg_alpha': 4.95885108498113e-05, 'reg_lambda': 0.10620592668788614, 'min_data_in_leaf': 86, 'feature_fraction': 0.9885774934760958, 'bagging_fraction': 0.7412056800719447, 'bagging_freq': 3}. Best is trial 0 with value: 0.6704280886415624.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's stratified-c-index: 0.668786\n",
      "5 folds stratified C-index: [np.float64(0.6696837215682377), np.float64(0.6637399361917257), np.float64(0.6715378111677521), np.float64(0.661494319595486), np.float64(0.66878574571599)]\n"
     ]
    }
   ],
   "source": [
    "# Now create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=3, n_jobs=1)\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6704280886415624"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model as Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "train = X\n",
    "test = X_test\n",
    "FEATURES = X.columns\n",
    "print(len(FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using XGBoost version 2.1.3\n",
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "[0]\tvalidation_0-auc:0.65127\n",
      "[100]\tvalidation_0-auc:0.72622\n",
      "[200]\tvalidation_0-auc:0.73465\n",
      "[300]\tvalidation_0-auc:0.73841\n",
      "[400]\tvalidation_0-auc:0.74028\n",
      "[500]\tvalidation_0-auc:0.74112\n",
      "[597]\tvalidation_0-auc:0.74135\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "[0]\tvalidation_0-auc:0.67067\n",
      "[100]\tvalidation_0-auc:0.75028\n",
      "[200]\tvalidation_0-auc:0.75782\n",
      "[300]\tvalidation_0-auc:0.76242\n",
      "[400]\tvalidation_0-auc:0.76424\n",
      "[500]\tvalidation_0-auc:0.76544\n",
      "[600]\tvalidation_0-auc:0.76587\n",
      "[681]\tvalidation_0-auc:0.76621\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "[0]\tvalidation_0-auc:0.66956\n",
      "[100]\tvalidation_0-auc:0.74831\n",
      "[200]\tvalidation_0-auc:0.75756\n",
      "[300]\tvalidation_0-auc:0.76059\n",
      "[400]\tvalidation_0-auc:0.76242\n",
      "[500]\tvalidation_0-auc:0.76330\n",
      "[600]\tvalidation_0-auc:0.76422\n",
      "[644]\tvalidation_0-auc:0.76401\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "[0]\tvalidation_0-auc:0.66334\n",
      "[100]\tvalidation_0-auc:0.74729\n",
      "[200]\tvalidation_0-auc:0.75507\n",
      "[300]\tvalidation_0-auc:0.75915\n",
      "[400]\tvalidation_0-auc:0.76168\n",
      "[500]\tvalidation_0-auc:0.76319\n",
      "[600]\tvalidation_0-auc:0.76427\n",
      "[700]\tvalidation_0-auc:0.76464\n",
      "[800]\tvalidation_0-auc:0.76511\n",
      "[835]\tvalidation_0-auc:0.76499\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "[0]\tvalidation_0-auc:0.66334\n",
      "[100]\tvalidation_0-auc:0.73907\n",
      "[200]\tvalidation_0-auc:0.74760\n",
      "[300]\tvalidation_0-auc:0.75125\n",
      "[400]\tvalidation_0-auc:0.75266\n",
      "[500]\tvalidation_0-auc:0.75321\n",
      "[600]\tvalidation_0-auc:0.75349\n",
      "[605]\tvalidation_0-auc:0.75345\n",
      "Accuracy: 0.6780\n",
      "F1 Score: 0.7470\n",
      "ROC AUC Score: 0.6606\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost\n",
    "print(\"Using XGBoost version\",xgboost.__version__)\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "FOLDS = 5\n",
    "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_xgb = np.zeros(len(train))\n",
    "pred_efs = np.zeros(len(test))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train, y_event)):\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {i+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    x_train = train.loc[train_index, FEATURES].copy()\n",
    "    y_train = y_event.loc[train_index]\n",
    "    x_valid = train.loc[test_index, FEATURES].copy()\n",
    "    y_valid = y_event.loc[test_index]\n",
    "    x_test = test[FEATURES].copy()\n",
    "\n",
    "    model_xgb = XGBClassifier(\n",
    "        device=\"cuda\",\n",
    "        max_depth=3,  \n",
    "        colsample_bytree=0.7129400756425178, \n",
    "        subsample=0.8185881823156917, \n",
    "        n_estimators=20_000, \n",
    "        learning_rate=0.04425768131771064,  \n",
    "        eval_metric=\"auc\", \n",
    "        early_stopping_rounds=50, \n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=1.5379160847615545,  \n",
    "        min_child_weight=4,\n",
    "        enable_categorical=True,\n",
    "        gamma=3.1330719334577584\n",
    "    )\n",
    "    model_xgb.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_valid, y_valid)],  \n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    # INFER OOF (Probabilities -> Binary)\n",
    "    oof_xgb[test_index] = (model_xgb.predict_proba(x_valid)[:, 1] > 0.5).astype(int)\n",
    "    # INFER TEST (Probabilities -> Average Probs)\n",
    "    pred_efs += model_xgb.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_efs = (pred_efs / FOLDS > 0.5).astype(int)\n",
    "\n",
    "# EVALUATE PERFORMANCE\n",
    "accuracy = accuracy_score(y_event, oof_xgb)\n",
    "f1 = f1_score(y_event, oof_xgb)\n",
    "roc_auc = roc_auc_score(y_event, oof_xgb)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
